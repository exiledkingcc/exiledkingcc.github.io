---
date:     2017-04-17
title:    字符集与编码这么混乱怎么想都是历史的错
summary:  字符集与编码的若干历史问题
tags: ["charset"]
categories: ["coding"]
---

首先，看到之前的一篇是在2016-10-22时写的，看来治疗懒癌是果断失败了。

写这一篇主要是来讲一讲字符集与编码的问题，因为我发现好多人搞不清楚这个问题。不过这也难免，毕竟这里面太多历史问题了。
所以这篇从历史的进程来讲这个问题，希望能够讲清楚。

现在的计算机都是32bit或者64bit的，一字节是8bit。有些用于嵌入式的芯片可能是8bit或者16bit的。
然而在20世纪70年代（1970）之前，计算机是各种bit都有的。俄罗斯还搞过三进制的计算机。
后来主要就是一字节8bit的。

那个时候的编码我也不知道是什么样的，找了一些资料，看到了几种互相都不一样的。这个时候大概是非常混乱的吧。

---

然后ASCII码诞生了，ASCII在1960年10月开始提出，1963年开始发布出版，1967年主版本，1986年最后一次修订。
ASCII定义了总共128个字符，其中33个不可见的控制字符，0-9阿拉伯数字，大小写的26英文字母，以及一些符号。

```
Dec Hex    Dec Hex    Dec Hex  Dec Hex  Dec Hex  Dec Hex   Dec Hex   Dec Hex
  0 00 NUL  16 10 DLE  32 20    48 30 0  64 40 @  80 50 P   96 60 `  112 70 p
  1 01 SOH  17 11 DC1  33 21 !  49 31 1  65 41 A  81 51 Q   97 61 a  113 71 q
  2 02 STX  18 12 DC2  34 22 "  50 32 2  66 42 B  82 52 R   98 62 b  114 72 r
  3 03 ETX  19 13 DC3  35 23 #  51 33 3  67 43 C  83 53 S   99 63 c  115 73 s
  4 04 EOT  20 14 DC4  36 24 $  52 34 4  68 44 D  84 54 T  100 64 d  116 74 t
  5 05 ENQ  21 15 NAK  37 25 %  53 35 5  69 45 E  85 55 U  101 65 e  117 75 u
  6 06 ACK  22 16 SYN  38 26 &  54 36 6  70 46 F  86 56 V  102 66 f  118 76 v
  7 07 BEL  23 17 ETB  39 27 '  55 37 7  71 47 G  87 57 W  103 67 g  119 77 w
  8 08 BS   24 18 CAN  40 28 (  56 38 8  72 48 H  88 58 X  104 68 h  120 78 x
  9 09 HT   25 19 EM   41 29 )  57 39 9  73 49 I  89 59 Y  105 69 i  121 79 y
 10 0A LF   26 1A SUB  42 2A *  58 3A :  74 4A J  90 5A Z  106 6A j  122 7A z
 11 0B VT   27 1B ESC  43 2B +  59 3B ;  75 4B K  91 5B [  107 6B k  123 7B {
 12 0C FF   28 1C FS   44 2C ,  60 3C <  76 4C L  92 5C \  108 6C l  124 7C |
 13 0D CR   29 1D GS   45 2D -  61 3D =  77 4D M  93 5D ]  109 6D m  125 7D }
 14 0E SO   30 1E RS   46 2E .  62 3E >  78 4E N  94 5E ^  110 6E n  126 7E ~
 15 0F SI   31 1F US   47 2F /  63 3F ?  79 4F O  95 5F _  111 6F o  127 7F DEL
```

在这段时期还有很多其他的编码，比如IBM的[EBCDIC](https://en.wikipedia.org/wiki/EBCDIC)。
这些都是用来编码英语的，而且互相并不兼容。比如EBCDIC编码中没有`{`、`}`等字符。熟悉C/C++的
应该都知道，可以用`??<`表示`{`，用`??>`表示`}`，就是为了兼容这种编码。由此还带来了一个
可能令人困惑的潜在的bug：

```c++
// is this right???/
x += 2;
```

C/C++会将`??/`解释成`\`，这样下面的`x += 2;`就成了注释的一部分。

---

言归正传。英语的编码方案成熟后，计算机的使用也从美国扩展到了全世界，首先就是欧洲。欧洲有很多
不同的语言，比如法语、德语、意大利语等，这些语言虽说与英语都是从拉丁语变化而来的，十分相似，
使用的字符很多都是一样的，但是也有部分字符是不同的，需要另外的表示。况且还有像希腊语这样与
英语差别很大的语言。这样ASCII码显然是没有办法处理的。怎么办呢？

**方法就是扩展ASCII码**。

ASCII码字符有128个，一共用了一个字节8bit中的7bit，还有一位没用。扩展ASCII就是从这剩余的1bit入手，
这一位就多了127个字符，对于使用字母符号的语言来说已经够用了。然而对于一种，或者相似度很多的几种
语言128个字符也许够用，但是要想把这些全部放一起，128就显得很少了。于是对与ASCII的扩展有很多，这些
共同形成了一个标准[ISO/IEC 8859](https://en.wikipedia.org/wiki/ISO/IEC_8859)。

`ISO/IEC 8859`是一个8bit的字符编码标准，不同于ASCII的一张表，它由很多部分组成。比如`ISO/IEC 8859-1`，
也就是`Latin-1`，包含了德语、意大利语、拉丁语等语言。比如`ISO/IEC 8859-7`，也称为`Latin/Greek`，包含了
希腊语。

---

可以看到，此时的ASCII码扩展方式都是考虑单字节的编码方式。显然一个字节的编码空间太小，对于字母语言或许用
上述蛋疼的方式还可以勉强使用，但是对于东亚的语言，特别是汉语，就完全不行了。计算机到了亚洲，到了中国后，
当地的字符怎么表示呢？还是**扩展ASCII码**。汉字这么多，显然一个字节8bit是装不下的，没办法只能用两个字节了
（当然两个也是不够的，不过这是后话了）。这就是[GB-2312](https://zh.wikipedia.org/wiki/GB_2312)了。

GB-2312中的GB是国家标准（`Guojia Biaozhun`）的意思。GB-2312为了能表示数量巨大的汉字，使用了两个字节，
又为了兼容ASCII，实际上是一种**变长编码**。第一个字节的范围在0x00-0x7f时，就和ASCII一样，只占一位，
其它的情况占两位。不过要注意，并不能取0x00-0xff所有的值，有些值是非法的。GB 2312标准共收录6763个汉字，
其中一级汉字3755个，二级汉字3008个；同时收录了包括拉丁字母、希腊字母等在内的682个字符，覆盖了大部分常用汉字，
然而还是有一些没有收录的，后来的GBK是对GB-2312的一种补充，增加了很多内容，并且兼容GB-2312。有意思的是：
GBK并不是国家标准，只是“技术规范指导性文件”，然而现实中的实现大多是都是GBK而不是GB-2312。

台湾使用的Big5编码与日本的Shift_JIS编码，与GB系列编码类似，只是映射的字符与具体的编码方式不一样。

---

按照以上的方式，世界上常用的语言文字都已经在计算机中编码了，投入使用了。这些都是`language-specific encodings`。
然而，全球化的潮流要求能够方便地使用多种语言文字，如果使用上述的这么多编码去实现这个目标就很蛋疼了。首先是系统的支持
问题，要一个系统同时支持这么多的编码是很大的工作量。其次，即便你的系统支持，你必须要标记文本它正确的编码是什么，
然后去解码，不然就乱码了。并且不同的编码不能放在同一份文本中。

认识到这个问题后，一些有志之士就开始着手解决这个问题，其思路就是把世界上所有文字、符号放到一个编码方案中统一起来。
这样上边的问题就迎刃而解了。这里也有些意思，因为有两拨人同时搞这个东西.一个是国际标准化组织ISO的`ISO/IEC JTC 1/SC 2`，
他们过了一个`ISO/IEC 10646`标准，也就是[UCS](https://en.wikipedia.org/wiki/Universal_Coded_Character_Set)。
另一个是一些计算机相关厂商的Unicode联盟（`The Unicode Consortium`），他们搞了一个[Unicode](https://en.wikipedia.org/wiki/Unicode)标准。

显然，为了统一各种语言的编码，最后反而搞出两个标准，这就会非常尴尬了。`ISO/IEC`与`Unicode`他们两拨人也认识到这一点，于是
统一了两个标准。现在，这两种标准都是存在的，并且都在修订与更新，只不过都是一样的。

看起来，字符编码的问题得到了解决，实际上并不然，顶多算是解决了50%。

---

这里要先将字符编码这个概念拆开来看，分别是字符与编码。

首先在计算机中只有0和1，每个是一个bit。由8个bit（0或1）构成了一个字节（byte）。然后以n个byte为单元，有了不同位的整数，
比如32位整数，64位整数。具体的看，计算机中都是0和1构成二进制，抽象看，就是一系列整数。
字符编码就是字符的二进制表示。这里就存在两个问题，一是这个字符映射到什么整数，二是这个整数要怎么用二进制表示。
这就是字符集与编码方式的概念。在Unicode之前，这两个概念没有严格分开，每一种`language-specific encodings`都是
字符集与编码方式绑在一起。而Unicode则是一种字符集对应多种编码方式。

Unicode最开始的时候，是一种16bit的编码方案。而ISO/IEC一开始就是设计了字符集，只不过实际中只用到了前面的65536个编码，
这种编码方案成为UCS-2，也是用两个字节16bit来编码，后来发现16bit的编码空间不够了，提出了UCS-4的32bit编码方案。
而Unicode一开始是固定的16bit的编码，后来两者合并了之后，Unicode的概念发生变化了，成为了一种字符集，
而原始的Unicode的编码方案就是没有代理对（`surrogate pairs`）的UTF-16。现在的UTF-16使用`surrogate pairs`，变成了变长编码，
可以表示整个Unicode字符集。但是由于这些历史原因，Unicode的意义在不同的语境下意思不同。
现在谈到Unicode一般就是指Unicode字符集，谈到具体的编码方式，会使用UTF-8、UTF-16等，
但是有一些地方在编码的时候也是用Unicode这个词，一般指的是UTF-16。

现在，我们在使用时最好统一概念，Unicode就表示Unicode字符集，不表示任何编码方案，使用UTF系列来表示各种具体编码方案，
不要再使用已经过时的概念了。

---

Unicode将世界各地的字符，符号进行了统一的编码，用一个整数表示，称为一个码点（`code point`）。
目前Unicode中`code point`的范围是0-0x10ffff，总共被分为17个部分，称为字符映射平面（`plane`），
每一平面有65536（2<sup>16</sup>）个`code point`。
其中第0号plane被称为基本多文种平面（`Basic Multilingual Plane`，`BMP`），包含了世界各地的常用文字、符号。
其余的plane称为补充平面，是对BMP的补充。比如常见的汉字符号就在BMP中，不常用的在其它平面中。
除了文字、数字字符等，Unicode还编码了很多其它的符号，比如emoji中的😂的`code point`是U+1F602。还有`surrogate pairs`，
不用来编码任何符号，而是作为组合来使用。

---

然而有了Unicode，还只是解决了字符集的问题。还有编码这个坑。Unicode与前面提到的GBK之类的不同，它一个字符集有多种编码方案。
现在广泛使用的主要是UTF-8、UTF-16和UTF-32这三种。

最开始是[UTF-16](https://en.wikipedia.org/wiki/UTF-16)，固定的16bit编码，
后来加上了`surrogate pairs`，成为变长编码，它以16bit为一个单元 。具体的编码方案如下：

| code ponit range | first 16bit unit | second 16bit unit  |
| ---------------- | ---------------- | ------------------ |
| U+0000 - U+D7FF  | 0x0000 - 0xD7FF  | |
| U+D800 - U+DBFF  | | |
| U+DC00 - U+DFFF  | | |
| U+E000 - U+FFFF  | 0xE000 - 0xFFFF  | |
| U+10000 - U+10FFFF | 0xD800 - 0xDBFF | 0xDC00 - 0xDFFF |

* `U+0000-U+D7FF`与`U+E000-U+FFFF`这个范围内的code points，用一个16bit的单位表示，值与code point相同；
* `U+D800-U+DFFF`是`surrogate pairs`；
* `U+10000-U+10FFFF`这个范围内的用两个16bit单元表示，也就是 `surrogate pairs`，计算方式如下：
  - 先减去0x10000，剩下的一共是20bit，分成高低各10bit
  - 高位10bit加上0xD800，成为first 16bit unit，也叫做high surrogate
  - 低位10bit加上0xDC00，成为second 16bit unit，也叫做low surrogate
  - 计算代码如下：

```c++
inline std::uint32_t surrogate_combine(std::uint32_t high, std::uint32_t low) {
  return 0x10000 + ((high & 0x3ff) << 10 | (low & 0x3ff));
}

inline void surrogate_split(std::uint32_t u, std::uint32_t& high, std::uint32_t& low) {
  u -= 0x10000;
  low = (u & 0x3ff) | 0xdc00;
  high = ((u >> 10) & 0x3ff) | 0xd800;
}
```

[UTF-32](https://en.wikipedia.org/wiki/UTF-32)就是32bit一个单元，目前Unicode的所有`code point`都不超过2<sup>32</sup>，
所以一个`code point`可以只使用一个32bit单元表示，就是一个uint32的值。

[UTF-8](https://en.wikipedia.org/wiki/UTF-8)以8bit，也就是1byte为一个单元。显然UTF-8也是变长的。编码方案如下：

| code point range | Byte 1 | Byte 2 | Byte 3 | Byte 4 |
| ---------------- | ------ | ------ | ------ | ------ |
| U+0000 - U+007F  | 0b0xxxxxxx |  |  |  |
| U+0080 - U+07FF  | 0b110xxxxx | 0b10xxxxxx |  |  |
| U+0800 - U+FFFF  | 0b1110xxxx | 0b10xxxxxx | 0b10xxxxxx	|
| U+10000 - U+10FFFF | 0b11110xxx | 0b10xxxxxx | 0b10xxxxxx | 0b10xxxxxx |

* UTF-8完全兼容ASCII，ASCII文本同时也是合法有效的UTF-8文本
* 以第一个字节的大小可以判断有几个字节
* 后面字节的最高两位都是10，用于同步纠错

UTF-8是目前Web以及其它网络传输使用最多的编码。

![Utf8webgrowth](/img/Utf8webgrowth.svg)

---

UTF-8使用如此之多，主要原因一是Unicode被广泛接受，二是相对于UTF-16，UTF3-2，UTF-8没有字节序问题。

字节序（[Endianness](https://en.wikipedia.org/wiki/Endianness)）就是字节在内存中以怎样的顺序存放。
内存的基本单元是1byte（8bit），16bit就是2byte，32bit就是4byte。一个16bit的数字，比如0x12345678，就是0x1234（高位）与0x5678（低位）两个字节。
这两个字节在内存中谁放在前面（内存低位）谁放在后面（内存高位），这是一个问题。字节序规定了这个顺序。
字节序有两种，数字高位放在内存高位的是“小端”字节序，反过来，数字高位放在内存低位的是“大端”字节序。
这“大小端”的名字来源也有些意思，不过这里就不讲了。

UTF-8的基本单元就是1byte，不存在字节序问题。UTF-16与UTF-32都有字节序问题，这两种编码各自都有小端（LE）与大端（BE）两种，一共四种：
UTF-16LE、UTF-16BE、UTF-16LE、UTF-16BE。

为了分辨一个文本是大端的还是小端的，Unicode规定可以在最前面放一个特定的标记，
这就是字节序标记（Byte order marker， [BOM](https://en.wikipedia.org/wiki/Byte_order_mark)）。
Unicode定义了一个`code point`来表示字节序，值为U+FEFF。
UTF-16最前面两个字节是0xff、0xfe时是UTF-16LE，是0xfe、0xff时是UTF-16BE。
UTF-32最前面四个字节是0xff、0xfe、0x00、0x00时是UTF-32LE，是0x00、0x00、0xfe、0xff时是UTF-32BE。
UTF-8不需要BOM，不过也可以有BOM，三个字节：0xef、0xbb、0xbf。

---

再说一下各种编程语言中的编码情况。

C/C++太老了，那个时候还不怎么关心编码问题，C/C++中的char基本上就是byte。

* char是byte，char*就是字节流，与字符集、编码都没为关系，源文件是什么编码，内容中就是什么样的。
* C++中的`string`就是封装了的char数组。
* C/C++中的宽字符`wchar_t`是个很尴尬的东西，标准就是规定可以存储更大的字符集，这个最大的字符集就是locale中的字符集，
也就是说，`wchar_t`实际上可以和char一样大。另外，`wchar_t`是`compiler-specific`，它到底有多少个bit都是不确定的。
* C11/C++11增加的`char16_t`和`char32_t`，就是16bit和32bit的字符，就是前面说的UTF-16的一个单元和UTF-32的一个单元。
* C++11中的`u16string`与`u32string`就是就是封装了的`char16_t`和`char32_t`数组。

再说java，Java中的char就是16bit的字符，和C11中的`char16_t`是一样的。Java中的String就是UTF-16。

对于C/C++中的字符串，基本上就是没法当做字符串用，只能当字节流用。比如下面的代码：

```c++
// C
char *ss = "hello 世界";
strlen(ss) = ?

// C++
string ss = "hello 世界";
ss.size() = ?
```

不管什么，直接给出一个数字都是错的。取决于的源文件编码：

* 要是UTF-8，6+2*3=12
* 要是UTF-16，8*2=16
* 要是UTF-32，8*4=32
* 要是GBK，6+2*2=10

光是一个算字符数就这么蛋疼，更何况其它的字符操作。比如把“第一名”改成“第二名”，你要改多少个字节，从第几个开始该呢？

对于Java就简单了，上面的字符操作都比较简单，不过遇到需要用`surrogate pairs`表示的，就有点麻烦了。
虽然对于字符操作问题不大，但是遇到需要处理其它编码的情况时，就变得和C/C++一样了，因为Java中的Char与String就只是UTF-16。
一般都是这样处理：
* 将其它编码的数据看做字节流，然后将该编码字的节流转换成UTF-16表示的字节流
* 将UTF-16表示的字节流转换成String（内容没变），进行字符文本的处理
* 处理完了后将String转换成UTF-16表示的字节流
* 最后将UTF-16表示的字节流转换成原来编码表示的字节流
当然，Java提供了`String.getBytes(Charset)`以及`byte[].toString(Charset)`这样简单API封装了上述过程，使用起来很简单。
虽然函数签名是`String`与`byte[]`的转换，不过本质上是字节流的转换。例如下面将String转换成UTF-8编码的字节流的代码。
（[UTF_8.Encoder.encodeArrayLoop](http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/sun/nio/cs/UTF_8.java#UTF_8.Encoder.encodeArrayLoop)）

```java
    private CoderResult encodeArrayLoop(CharBuffer src,
                                        ByteBuffer dst)
    {
        char[] sa = src.array();
        int sp = src.arrayOffset() + src.position();
        int sl = src.arrayOffset() + src.limit();

        byte[] da = dst.array();
        int dp = dst.arrayOffset() + dst.position();
        int dl = dst.arrayOffset() + dst.limit();
        int dlASCII = dp + Math.min(sl - sp, dl - dp);

        //ASCII only loop
        while (dp < dlASCII && sa[sp] < '\u0080')
            da[dp++] = (byte) sa[sp++];
        while (sp < sl) {
            int c = sa[sp];
            if (c < 0x80) {
                // Have at most seven bits
                if (dp >= dl)
                    return overflow(src, sp, dst, dp);
                da[dp++] = (byte)c;
            } else if (c < 0x800) {
                // 2 bytes, 11 bits
                if (dl - dp < 2)
                   return overflow(src, sp, dst, dp);
                da[dp++] = (byte)(0xc0 | ((c >> 06)));
                da[dp++] = (byte)(0x80 | (c & 0x3f));
            } else if (Surrogate.is(c)) {
                // Have a surrogate pair
                if (sgp == null)
                    sgp = new Surrogate.Parser();
                int uc = sgp.parse((char)c, sa, sp, sl);
                if (uc < 0) {
                    updatePositions(src, sp, dst, dp);
                    return sgp.error();
                }
                if (dl - dp < 4)
                    return overflow(src, sp, dst, dp);
                da[dp++] = (byte)(0xf0 | ((uc >> 18)));
                da[dp++] = (byte)(0x80 | ((uc >> 12) & 0x3f));
                da[dp++] = (byte)(0x80 | ((uc >> 06) & 0x3f));
                da[dp++] = (byte)(0x80 | (uc & 0x3f));
                sp++;  // 2 chars
            } else {
                // 3 bytes, 16 bits
                if (dl - dp < 3)
                    return overflow(src, sp, dst, dp);
                da[dp++] = (byte)(0xe0 | ((c >> 12)));
                da[dp++] = (byte)(0x80 | ((c >> 06) & 0x3f));
                da[dp++] = (byte)(0x80 | (c & 0x3f));
            }
            sp++;
        }
        updatePositions(src, sp, dst, dp);
        return CoderResult.UNDERFLOW;
    }
```

再说说Python。Python2里面字符编码也是和前面一样，bytes、str还有Unicode的概念坑了很多人。Python3就非常清晰了。
Python3里面就是`str`：字符串（`Unicode code point sequence`），`bytes`：字节串，两者通过`encode`和`decode`来相互转换。
至于其内部表示，就不是Java里面直接用UTF-16，而是比较复杂的复合类型。主要是考虑单字节或者16bit一个单元不方便处理non-BMP部分，而32bit又浪费空间。
详见[PEP 393](https://www.python.org/dev/peps/pep-0393/)。源代码（`Include/unicodeobje.h`）中定义如下：

```c
/* ASCII-only strings created through PyUnicode_New use the PyASCIIObject
   structure. state.ascii and state.compact are set, and the data
   immediately follow the structure. utf8_length and wstr_length can be found
   in the length field; the utf8 pointer is equal to the data pointer. */
typedef struct {
    /* There are 4 forms of Unicode strings:

       - compact ascii:

         * structure = PyASCIIObject
         * test: PyUnicode_IS_COMPACT_ASCII(op)
         * kind = PyUnicode_1BYTE_KIND
         * compact = 1
         * ascii = 1
         * ready = 1
         * (length is the length of the utf8 and wstr strings)
         * (data starts just after the structure)
         * (since ASCII is decoded from UTF-8, the utf8 string are the data)

       - compact:

         * structure = PyCompactUnicodeObject
         * test: PyUnicode_IS_COMPACT(op) && !PyUnicode_IS_ASCII(op)
         * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or
           PyUnicode_4BYTE_KIND
         * compact = 1
         * ready = 1
         * ascii = 0
         * utf8 is not shared with data
         * utf8_length = 0 if utf8 is NULL
         * wstr is shared with data and wstr_length=length
           if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2
           or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_t)=4
         * wstr_length = 0 if wstr is NULL
         * (data starts just after the structure)

       - legacy string, not ready:

         * structure = PyUnicodeObject
         * test: kind == PyUnicode_WCHAR_KIND
         * length = 0 (use wstr_length)
         * hash = -1
         * kind = PyUnicode_WCHAR_KIND
         * compact = 0
         * ascii = 0
         * ready = 0
         * interned = SSTATE_NOT_INTERNED
         * wstr is not NULL
         * data.any is NULL
         * utf8 is NULL
         * utf8_length = 0

       - legacy string, ready:

         * structure = PyUnicodeObject structure
         * test: !PyUnicode_IS_COMPACT(op) && kind != PyUnicode_WCHAR_KIND
         * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or
           PyUnicode_4BYTE_KIND
         * compact = 0
         * ready = 1
         * data.any is not NULL
         * utf8 is shared and utf8_length = length with data.any if ascii = 1
         * utf8_length = 0 if utf8 is NULL
         * wstr is shared with data.any and wstr_length = length
           if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2
           or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_4)=4
         * wstr_length = 0 if wstr is NULL

       Compact strings use only one memory block (structure + characters),
       whereas legacy strings use one block for the structure and one block
       for characters.

       Legacy strings are created by PyUnicode_FromUnicode() and
       PyUnicode_FromStringAndSize(NULL, size) functions. They become ready
       when PyUnicode_READY() is called.

       See also _PyUnicode_CheckConsistency().
    */
    PyObject_HEAD
    Py_ssize_t length;          /* Number of code points in the string */
    Py_hash_t hash;             /* Hash value; -1 if not set */
    struct {
        /*
           SSTATE_NOT_INTERNED (0)
           SSTATE_INTERNED_MORTAL (1)
           SSTATE_INTERNED_IMMORTAL (2)

           If interned != SSTATE_NOT_INTERNED, the two references from the
           dictionary to this object are *not* counted in ob_refcnt.
         */
        unsigned int interned:2;
        /* Character size:

           - PyUnicode_WCHAR_KIND (0):

             * character type = wchar_t (16 or 32 bits, depending on the
               platform)

           - PyUnicode_1BYTE_KIND (1):

             * character type = Py_UCS1 (8 bits, unsigned)
             * all characters are in the range U+0000-U+00FF (latin1)
             * if ascii is set, all characters are in the range U+0000-U+007F
               (ASCII), otherwise at least one character is in the range
               U+0080-U+00FF

           - PyUnicode_2BYTE_KIND (2):

             * character type = Py_UCS2 (16 bits, unsigned)
             * all characters are in the range U+0000-U+FFFF (BMP)
             * at least one character is in the range U+0100-U+FFFF

           - PyUnicode_4BYTE_KIND (4):

             * character type = Py_UCS4 (32 bits, unsigned)
             * all characters are in the range U+0000-U+10FFFF
             * at least one character is in the range U+10000-U+10FFFF
         */
        unsigned int kind:3;
        /* Compact is with respect to the allocation scheme. Compact unicode
           objects only require one memory block while non-compact objects use
           one block for the PyUnicodeObject struct and another for its data
           buffer. */
        unsigned int compact:1;
        /* The string only contains characters in the range U+0000-U+007F (ASCII)
           and the kind is PyUnicode_1BYTE_KIND. If ascii is set and compact is
           set, use the PyASCIIObject structure. */
        unsigned int ascii:1;
        /* The ready flag indicates whether the object layout is initialized
           completely. This means that this is either a compact object, or
           the data pointer is filled out. The bit is redundant, and helps
           to minimize the test in PyUnicode_IS_READY(). */
        unsigned int ready:1;
        /* Padding to ensure that PyUnicode_DATA() is always aligned to
           4 bytes (see issue #19537 on m68k). */
        unsigned int :24;
    } state;
    wchar_t *wstr;              /* wchar_t representation (null-terminated) */
} PyASCIIObject;

/* Non-ASCII strings allocated through PyUnicode_New use the
   PyCompactUnicodeObject structure. state.compact is set, and the data
   immediately follow the structure. */
typedef struct {
    PyASCIIObject _base;
    Py_ssize_t utf8_length;     /* Number of bytes in utf8, excluding the
                                 * terminating \0. */
    char *utf8;                 /* UTF-8 representation (null-terminated) */
    Py_ssize_t wstr_length;     /* Number of code points in wstr, possible
                                 * surrogates count as two code points. */
} PyCompactUnicodeObject;

/* Strings allocated through PyUnicode_FromUnicode(NULL, len) use the
   PyUnicodeObject structure. The actual string data is initially in the wstr
   block, and copied into the data block using _PyUnicode_Ready. */
typedef struct {
    PyCompactUnicodeObject _base;
    union {
        void *any;
        Py_UCS1 *latin1;
        Py_UCS2 *ucs2;
        Py_UCS4 *ucs4;
    } data;                     /* Canonical, smallest-form Unicode buffer */
} PyUnicodeObject;
```

---

**to be continued…**


